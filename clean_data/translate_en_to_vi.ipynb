{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "436b4796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d09aa030a44e35883d223657251749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Lenovo\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-vi. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e23053fee8a43b2a9796907ba34f88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/809k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe4c0bd70f14917b8dd751b75f2d636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/756k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59f9384b682400cb1c3df69296a864a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d8cfba777e4d52b4e4f9e10ac3d570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae184d1d64e400a952e0eb7460f0fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/289M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87c9630519d4b599819c10350144b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b4e36bc841496391a0716a6dc477a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/289M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "MODEL_NAME = \"Helsinki-NLP/opus-mt-en-vi\"\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = MarianMTModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb64f8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/9242 [11:46<201:27:58, 78.55s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     28\u001b[39m         translated_texts.extend(translated)\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m translated_texts\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mtext_vi\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mtranslate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m df_vi = df[[\u001b[33m\"\u001b[39m\u001b[33mtext_vi\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33memotion\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrisk\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m     36\u001b[39m df_vi = df_vi.rename(columns={\u001b[33m\"\u001b[39m\u001b[33mtext_vi\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mtranslate_batch\u001b[39m\u001b[34m(texts, batch_size)\u001b[39m\n\u001b[32m     13\u001b[39m encoded = tokenizer(\n\u001b[32m     14\u001b[39m     batch,\n\u001b[32m     15\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     max_length=\u001b[32m128\u001b[39m\n\u001b[32m     19\u001b[39m ).to(device)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     generated = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m translated = tokenizer.batch_decode(\n\u001b[32m     25\u001b[39m     generated,\n\u001b[32m     26\u001b[39m     skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     27\u001b[39m )\n\u001b[32m     28\u001b[39m translated_texts.extend(translated)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\generation\\utils.py:2551\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2539\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._sample(\n\u001b[32m   2540\u001b[39m         input_ids,\n\u001b[32m   2541\u001b[39m         logits_processor=prepared_logits_processor,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2546\u001b[39m         **model_kwargs,\n\u001b[32m   2547\u001b[39m     )\n\u001b[32m   2549\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2550\u001b[39m     \u001b[38;5;66;03m# 11. run beam sample\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2552\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2553\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2554\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2555\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2556\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2557\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2558\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2560\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n\u001b[32m   2561\u001b[39m     logger.warning_once(\n\u001b[32m   2562\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGroup Beam Search is scheduled to be moved to a `custom_generate` repository in v4.55.0. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2563\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2564\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\generation\\utils.py:3462\u001b[39m, in \u001b[36mGenerationMixin._beam_search\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   3460\u001b[39m         model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._reorder_cache(model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m], beam_idx)\n\u001b[32m   3461\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3462\u001b[39m         \u001b[43mmodel_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpast_key_values\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3464\u001b[39m cur_len = cur_len + \u001b[32m1\u001b[39m\n\u001b[32m   3465\u001b[39m is_early_stop_heuristic_unsatisfied = \u001b[38;5;28mself\u001b[39m._check_early_stop_heuristic(\n\u001b[32m   3466\u001b[39m     is_early_stop_heuristic_unsatisfied=is_early_stop_heuristic_unsatisfied,\n\u001b[32m   3467\u001b[39m     running_beam_scores=running_beam_scores,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3474\u001b[39m     length_penalty=length_penalty,\n\u001b[32m   3475\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\cache_utils.py:1349\u001b[39m, in \u001b[36mEncoderDecoderCache.reorder_cache\u001b[39m\u001b[34m(self, beam_idx)\u001b[39m\n\u001b[32m   1347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreorder_cache\u001b[39m(\u001b[38;5;28mself\u001b[39m, beam_idx: torch.LongTensor):\n\u001b[32m   1348\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1349\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attention_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28mself\u001b[39m.cross_attention_cache.reorder_cache(beam_idx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\cache_utils.py:884\u001b[39m, in \u001b[36mCache.reorder_cache\u001b[39m\u001b[34m(self, beam_idx)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Reorder the cache for beam search\"\"\"\u001b[39;00m\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.layers)):\n\u001b[32m--> \u001b[39m\u001b[32m884\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\cache_utils.py:79\u001b[39m, in \u001b[36mCacheLayerMixin.reorder_cache\u001b[39m\u001b[34m(self, beam_idx)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_seq_length() > \u001b[32m0\u001b[39m:\n\u001b[32m     78\u001b[39m     \u001b[38;5;28mself\u001b[39m.keys = \u001b[38;5;28mself\u001b[39m.keys.index_select(\u001b[32m0\u001b[39m, beam_idx.to(\u001b[38;5;28mself\u001b[39m.keys.device))\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28mself\u001b[39m.values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "INPUT_FILE = \"../data/train/train.csv\"     # đổi thành train / val / test\n",
    "OUTPUT_FILE = \"../data/train/train_vi.csv\"\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "texts = df[\"text\"].tolist()\n",
    "\n",
    "\n",
    "def translate_batch(texts, batch_size=16):\n",
    "    translated_texts = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        encoded = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(**encoded)\n",
    "\n",
    "        translated = tokenizer.batch_decode(\n",
    "            generated,\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        translated_texts.extend(translated)\n",
    "    return translated_texts\n",
    "\n",
    "\n",
    "df[\"text_vi\"] = translate_batch(texts)\n",
    "\n",
    "\n",
    "df_vi = df[[\"text_vi\", \"emotion\", \"risk\"]]\n",
    "df_vi = df_vi.rename(columns={\"text_vi\": \"text\"})\n",
    "\n",
    "df_vi.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"✅ Translation completed:\", OUTPUT_FILE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
